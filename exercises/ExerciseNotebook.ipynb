{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Computer Vision Exercise Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Computer Images and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display test image\n",
    "\n",
    "# open test image\n",
    "dog_raw = cv2.imread('../data/dog.jpg')  # default: bgr for display\n",
    "print(f\"arr1 shape (H x W x Channels) or (Rows x Cols x Channels): {dog_raw.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dog_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dog_raw)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks a little...blue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use openCV to reverse channel order\n",
    "plt.imshow(cv2.cvtColor(dog_raw, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert channels a different way\n",
    "dog = dog_raw[..., ::-1]                  # bgr -> rgb for inference\n",
    "plt.imshow(dog)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulating the image is achieved by manipulating the array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropping the image is a simple as indexing the array\n",
    "cropped = dog[:100,:100,:]\n",
    "plt.imshow(cropped)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit color channels\n",
    "# Channel order: RGB\n",
    "no_green = dog.copy()\n",
    "no_green[:,:,1] = 0\n",
    "\n",
    "plt.imshow(no_green)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimming is a matter of division!\n",
    "dimmed = (dog.copy() / 2).astype(int)\n",
    "\n",
    "plt.imshow(dimmed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first 20 elements of the first row of each image's Green channel\n",
    "print(dimmed[0,:20,1])\n",
    "print(dog[0,:20,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we multiply?\n",
    "doubled = (dog.copy() * 2).astype(int)\n",
    "\n",
    "plt.imshow(doubled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huh? Some are brighter but some are dimmer\n",
    "# inspect the 200-215th elements of the first row of each image's Green channel\n",
    "print(dog[0,:15,1])\n",
    "print(doubled[0,:15,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the 200-215th elements of the first row of each image's Green channel\n",
    "print(dog[0,200:215,1])\n",
    "print(doubled[0,200:215,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = np.array([188, 204, 240, 236, 224, 224, 218, 203, 186, 231])\n",
    "doubled_for_real = original * 2\n",
    "double_modulo = (original * 2) % 256\n",
    "print(original)\n",
    "print(doubled_for_real)\n",
    "print(double_modulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't represent values over 255, so instead we're seeing (arr1 * 2) modulo 256\n",
    "# how does the computer know to do this atuomatically??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dog.dtype)\n",
    "print(original.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is the data type! The datatype of `arr2` is 'uint8' which stands for \"unsigned 8-bit integer\". When we initially read in this image to the variable `arr1` with the code `arr1 = cv2.imread('../data/dog.jpg')`, it was automatically encoded as 'uint8'.  This data type represents integers with 8 binary digits (bits). It ranges from 00000000 to 11111111 (which in decimal is 255). In other words this data type is only expressive enough to represent intergers in the range [0,255]. Furthermore if an operation results in a value greater than 255, than uint8 will \"wrap around\" using modulo arithmetic as we observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe instruct them to play around with the image here\n",
    "plt.imshow(dog)\n",
    "plt.show()\n",
    "\n",
    "#mini challenges/ exercises\n",
    "# can you think of an image transformation, and then implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letterbox procedure\n",
    "def letterbox(src, dest_shape):\n",
    "    ## INPUTS ##\n",
    "        # src - an image array\n",
    "        # dest_shape - tuple specifying desired letterboxed image dimensions\n",
    "    ## Output ##\n",
    "        # dest - letterboxed image array\n",
    "    \n",
    "    # get src dims\n",
    "    src_width = src.shape[1]    # img.shape returns tuple (rows, cols, chan)\n",
    "    src_height = src.shape[0]   # NOTE: rows => height; cols => width\n",
    "\n",
    "    # cons dest array (filled with gray), get dest dims\n",
    "    # NOTE: each 32-bit [B, G, R, A] pixel value is [128, 128, 128, 255]\n",
    "    dest = np.full(dest_shape, np.uint8(128))\n",
    "    if dest_shape[2] > 3:\n",
    "        dest[:, :, 3] = np.uint8(255)\n",
    "    dest_width = dest.shape[1]\n",
    "    dest_height = dest.shape[0]\n",
    "\n",
    "    # calculate width and height ratios\n",
    "    width_ratio = dest_width / src_width        # NOTE: ratios are float values\n",
    "    height_ratio = dest_height / src_height\n",
    "\n",
    "    # init resized image width and height with max values (dest dims)\n",
    "    rsz_width = dest_width\n",
    "    rsz_height = dest_height\n",
    "\n",
    "    # smallest scale factor will scale other dimension as well\n",
    "    if width_ratio < height_ratio:\n",
    "        rsz_height = int(src_height * width_ratio)  # NOTE: integer truncation\n",
    "    else:\n",
    "        rsz_width = int(src_width * height_ratio)\n",
    "\n",
    "    # resize the image data using bi-linear interpolation\n",
    "    rsz_dims = (rsz_width, rsz_height)\n",
    "    rsz = cv2.resize(src, rsz_dims, 0, 0, cv2.INTER_LINEAR)\n",
    "\n",
    "    # embed rsz into the center of dest\n",
    "    dx = int((dest_width - rsz_width) / 2)          # NOTE: integer truncation\n",
    "    dy = int((dest_height - rsz_height) / 2)\n",
    "    dest[dy:dy+rsz_height, dx:dx+rsz_width, :] = rsz\n",
    "    rsz_origin = (dx, dy)\n",
    "\n",
    "    # letterboxing complete, return dest\n",
    "    return (dest, rsz_origin, rsz_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# letterbox the image to resize for NN input (size: (height, width, chan))\n",
    "letterboxed_dog = letterbox(dog, (416, 416, 3))[0]\n",
    "plt.imshow(letterboxed_dog)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack_buffer procedure, ONNX model expects normalized float32 NCHW tensor\n",
    "def pack_buffer(src):\n",
    "    dest = np.array(src, dtype='float32')       # cons dest array via copy\n",
    "    if dest.shape[2] > 3:\n",
    "        dest = dest[:,:,:3]                     # if there is an alpha channel, remove it\n",
    "    #dest = dest [..., ::-1]                     # reorder channels BGR -> RGB\n",
    "    dest /= 255.0                               # normalize vals\n",
    "    dest = np.transpose(dest, [2, 0, 1])        # make channel first dim\n",
    "    dest = np.expand_dims(dest, 0)              # ins batch dim before chan dim\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_dog = pack_buffer(dog)\n",
    "buffered_dog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the values are no longer integers\n",
    "buffered_dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered_dog.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Float\" or floating-point numbers are used to represent real numbers (as opposed to integers--think: fractions and decimals). \"32\" refers to the number of bits allocated to store each floating-point number. 32 bits = 4 bytes.  \n",
    "Representing floating point numbers with a fixed number of bits entails a tradeoff between the range of values you can represent and their precision.  \n",
    "TODO: briefly talk about how floating point numbers are stored (sign, exponent, significand/mantissa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letterboxing the live camera feed\n",
    "\n",
    "# create a config with desired attributes: format, size, framerate\n",
    "# NOTE: camera resolution 4608x2464, downsamples at 2304x1296 (56.03 fps)\n",
    "# NOTE: XRGB8888 => shape: (height, width, 4); pixel value: [B, G, R, A]\n",
    "config = picam2.create_preview_configuration(\n",
    "    main={'format': 'XRGB8888', 'size': (2304, 1296)})  # 16:9 aspect ratio\n",
    "\n",
    "# set camera configuration, start camera\n",
    "picam2.configure(config)\n",
    "picam2.start()\n",
    "\n",
    "# start opencv window thread\n",
    "cv2.startWindowThread()\n",
    "wnd_name = 'foo'\n",
    "cv2.namedWindow(wnd_name, cv2.WINDOW_KEEPRATIO)\n",
    "cv2.resizeWindow(wnd_name, 416, 416)                    # 1:1 aspect ratio\n",
    "\n",
    "while True:\n",
    "    # get current image data from 'main' camera stream\n",
    "    arr1 = picam2.capture_array('main')\n",
    "\n",
    "    # letterbox the image to resize for NN input (size: (height, width, chan))\n",
    "    arr2 = letterbox(arr1, (416, 416, 4))[0]\n",
    "\n",
    "    # cons packed input buffer for ONNX model inference\n",
    "    arr3 = pack_buffer(arr2)\n",
    "    dim3 = np.array([arr2.shape[1],arr2.shape[0]],dtype=np.float32).reshape(1,2)\n",
    "\n",
    "    # if window closed, break loop before imshow creates new window\n",
    "    if cv2.getWindowProperty(wnd_name, cv2.WND_PROP_AUTOSIZE) == -1:\n",
    "        break\n",
    "\n",
    "    # show annotated image\n",
    "    # cv2.imshow(wnd_name, arr4)\n",
    "    cv2.imshow(wnd_name, arr2)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# stop camera\n",
    "cv2.destroyWindow(wnd_name)\n",
    "picam2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Pre-Trained Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Pre-Trained Neural Networks (PTNNs)\n",
    "    a.\tPTNNs have architecture and trained weights.  \n",
    "    b.\tGetting trained Tiny YOLOv3 from ONNX model zoo  \n",
    "    c.\tConsider NETRON model viewer (https://github.com/lutzroeder/netron)  \n",
    "    d.\tonnx2torch module  \n",
    "    e.\tLoading ONNX model into pytorch  \n",
    "    f.\tRun on a test image look at output: bbox center and extent, objectness, classifications.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the model classes\n",
    "def read_model_classes(pathname = '../model/coco.names'):\n",
    "    file = open(pathname, 'r')\n",
    "    classes = []\n",
    "    while True:\n",
    "        class_name = file.readline().strip()\n",
    "        if not class_name:\n",
    "            break\n",
    "        classes.append(class_name)\n",
    "    file.close()\n",
    "    return classes\n",
    "\n",
    "def run_inference(model, image_array):\n",
    "    # cons input for ONNX model inference (packed images and their orig dims)\n",
    "    img = pack_buffer(image_array)\n",
    "    # dim4 = np.array([image_array.shape[1], image_array.shape[0]], dtype=np.float32).reshape(1, 2)\n",
    "\n",
    "    # run ONNX model inference on input buffer to get results\n",
    "    return model.run(None, {'input_1': img })#,'image_shape': dim4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_names = read_model_classes()\n",
    "for item in coco_names:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cons ONNX Tiny YOLOv3 NN model\n",
    "model   = ort.InferenceSession('../model/modified_yolov3-tiny.onnx')\n",
    "# model = ort.InferenceSession('../model/yolov3-tiny.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_results1 = run_inference(model, letterboxed_dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our results!\n",
    "dog_results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dog_results1))     \n",
    "print(dog_results1[0].shape)\n",
    "print(dog_results1[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_results1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Bounding Boxes for Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_annos procedure (fixed ONNX anno scaling in unscale_annos proc)\n",
    "def draw_annos(src, annos, classes):\n",
    "    dest = np.copy(src)\n",
    "    green = (0, 255, 0)\n",
    "    black = (0, 0, 0)\n",
    "    face = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    scale = 0.5\n",
    "    thickness = 1\n",
    "    for anno in annos:\n",
    "        pt1 = (anno[0][0], anno[0][1])\n",
    "        pt2 = (anno[0][2], anno[0][3])\n",
    "        text = f'{classes[anno[2]]}: {anno[1]:.2f}'\n",
    "        (w, h), _ = cv2.getTextSize(text, face, scale, thickness)\n",
    "        pt3 = (pt1[0], pt1[1] - h)\n",
    "        pt4 = (pt1[0] + w, pt1[1])\n",
    "        dest = cv2.rectangle(dest, pt1, pt2, green)\n",
    "        dest = cv2.rectangle(dest, pt3, pt4, green, cv2.FILLED)\n",
    "        dest = cv2.putText(dest, text, pt1, face, scale, black, thickness)\n",
    "    return dest\n",
    "\n",
    "# unscale_annos procedure (fixes ONNX anno scaling)\n",
    "def unscale_annos(annos, dw, dh, w0, h0, w1, h1):\n",
    "    res = []\n",
    "    scale_w = float(w1) / float(w0)\n",
    "    scale_h = float(h1) / float(h0)\n",
    "    for anno in annos:\n",
    "        pt1 = (int(anno[0][1]), int(anno[0][0]))   # ONNX bug! Points are\n",
    "        pt2 = (int(anno[0][3]), int(anno[0][2]))   # transposed.\n",
    "        pt3 = (pt1[0] - dw, pt1[1] - dh)\n",
    "        pt4 = (pt2[0] - dw, pt2[1] - dh)\n",
    "        pt5 = (int(float(pt3[0]) * scale_w), int(float(pt3[1]) * scale_h))\n",
    "        pt6 = (int(float(pt4[0]) * scale_w), int(float(pt4[1]) * scale_h))\n",
    "        arr1 = np.array([pt5[0], pt5[1], pt6[0], pt6[1]], dtype='int32')\n",
    "        res.append((arr1, anno[1], anno[2]))\n",
    "    return res\n",
    "\n",
    "#+BEGIN_EXAMPLE\n",
    "\n",
    "# sigmoid procedure\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + math.exp(-x))\n",
    "\n",
    "# (redefined) proc_results procedure\n",
    "def proc_results(res, classes, pobj_thresh = 0.1, pcls_thresh = 0.5, orig_img_size = 416,\n",
    "                 anchors = np.array([[[81,82], [135,169], [344,319]],\n",
    "                                     [[10,14], [ 23, 27], [ 37, 58]]],\n",
    "                                    dtype='int32')):\n",
    "    dets = []\n",
    "    # candidate detection layout:\n",
    "    # [x, y, w, h, pobj, pcls_0, pcls_1, ..., pcls_i]\n",
    "    # i: [0, num_classes)\n",
    "    num_classes = len(classes)\n",
    "    pcls_offset = 5                                     # offset of class probs\n",
    "    num_params = pcls_offset + num_classes              # numParams per cand det\n",
    "    num_yolo_blocks = anchors.shape[0]\n",
    "    num_anchors = anchors.shape[1]\n",
    "    assert len(res) == num_yolo_blocks\n",
    "    for blk in range(num_yolo_blocks):                  # iter over yolo blocks\n",
    "        height_blk = res[blk].shape[1]\n",
    "        width_blk = res[blk].shape[2]\n",
    "        stride_blk = orig_img_size / width_blk          # ASSUMES square image\n",
    "        shape_blk = (height_blk, width_blk, num_anchors, num_params)\n",
    "        dets_blk = np.reshape(res[blk], shape_blk)\n",
    "        # each yolo block has an \"image\" where each \"pixel\" has a candidate\n",
    "        # detection per anchor box\n",
    "        for hi in range(height_blk):                    # iter over img rows\n",
    "            for wi in range(width_blk):                 # iter over img cols\n",
    "                for ai in range(num_anchors):           # iter over pxl anchors\n",
    "                    det = dets_blk[hi][wi][ai]          # get detection\n",
    "                    pobj = sigmoid(det[4])              # get objectness prob\n",
    "                    if pobj > pobj_thresh:\n",
    "                        x = stride_blk * (wi + sigmoid(det[0]))\n",
    "                        y = stride_blk * (hi + sigmoid(det[1]))\n",
    "                        w = math.exp(det[2]) * anchors[blk][ai][0]\n",
    "                        h = math.exp(det[3]) * anchors[blk][ai][1]\n",
    "                        for ci in range(num_classes):\n",
    "                            pcls = sigmoid(det[pcls_offset + ci])\n",
    "                            if pcls > pcls_thresh:\n",
    "                                x1, y1 = x - (w / 2.0), y - (h / 2.0)\n",
    "                                x2, y2 = x + (w / 2.0), y + (h / 2.0)\n",
    "                                dets.append((pobj, pcls, ci, x1, y1, x2, y2))\n",
    "    return dets\n",
    "\n",
    "# overlap procedure, find bbox overlap length along a dim\n",
    "def overlap(lo1, hi1, lo2, hi2):\n",
    "    lo = max(lo1, lo2)\n",
    "    hi = min(hi1, hi2)\n",
    "    return hi - lo\n",
    "\n",
    "# iou procedure (intersection-over-union); bbox: [xl, yl, xh, yh]\n",
    "def iou(bbox1, bbox2):\n",
    "    area1 = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])   # bbox1 area\n",
    "    area2 = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])   # bbox2 area\n",
    "    wo = overlap(bbox1[0], bbox1[2], bbox2[0], bbox2[2])    # overlap x dim\n",
    "    ho = overlap(bbox1[1], bbox1[3], bbox2[1], bbox2[3])    # overlap y dim\n",
    "    i_area = (wo * ho) if (wo > 0.0 and ho > 0.0) else 0.0  # intersection area\n",
    "    u_area = area1 + area2 - i_area                         # union area\n",
    "    return i_area / u_area\n",
    "\n",
    "# basic_nms procedure (non-maximum supression); det: (pobj,pcls,ci,x1,y1,x2,y2)\n",
    "def basic_nms(dets, iou_thresh = 0.5):\n",
    "    filtered_dets = []\n",
    "    dets.sort(reverse=True)                     # lexicographically sort dets\n",
    "    while len(dets) > 0:                        # any remaining dets to check?\n",
    "        c = dets[0]                             # get current det\n",
    "        filtered_dets.append(c)                 # add to filtered_dets\n",
    "        # predicate remove dets with same class index and high iou\n",
    "        pred = lambda d : not (c[2] == d[2] and iou(c[3:], d[3:]) > iou_thresh)\n",
    "        dets = [d for d in dets if pred(d)]     # make list of remaining dets\n",
    "    return filtered_dets\n",
    "\n",
    "# make_annos procedure\n",
    "def make_annos(dets):\n",
    "    annos = []\n",
    "    for det in dets:\n",
    "        box = [det[4], det[3], det[6], det[5]]  # NOTE: replicate ONNX bug\n",
    "        score = det[0] * det[1]\n",
    "        cls = det[2]\n",
    "        annos.append((box, score, cls))\n",
    "    return annos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h1, w1, c1) = dog.shape\n",
    "(letterboxed_dog, (dw, dh), (w0, h0)) = letterbox(dog, (416, 416, 3))\n",
    "dog_dets1 = proc_results(dog_results1, coco_names,0.05, 0.05)\n",
    "#filtered_dets = basic_nms(dog_dets1, 0.5)\n",
    "dog_annos1 = make_annos(dog_dets1)\n",
    "\n",
    "# # unscale annotations to draw in original image frame\n",
    "dog_unscaled1 = unscale_annos(dog_annos1, dw, dh, w0, h0, w1, h1)\n",
    "\n",
    "# # draw list of annotations on original image\n",
    "dog_annotated1 = draw_annos(dog_raw, dog_unscaled1, coco_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dog_annotated1[..., ::-1]  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(dog_unscaled1)} detections!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Post-processing YOLO Results with Non-Max Suppression (NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could have done even worse\n",
    "dets = proc_results(dog_results1, coco_names, pobj_thresh=0, pcls_thresh=0)\n",
    "print(f\"When we set the thresholds to 0, there are {len(dets)} detections!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why?\n",
    "# (13 * 13 cells) * (3 anchor boxes * 80 classes)\n",
    "(13 * 13 * 3 * 80) + (26 * 26 * 3 * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's set new thresholds and add NMS\n",
    "\n",
    "OBJ_THRESH = 0.05\n",
    "CLASS_THRESH = 0.05\n",
    "IOU_THRESH = 0.9\n",
    "\n",
    "dog_dets2 = proc_results(dog_results1, coco_names, pobj_thresh=OBJ_THRESH, pcls_thresh=CLASS_THRESH)\n",
    "filtered_dog_dets = basic_nms(dog_dets2,IOU_THRESH)\n",
    "dog_annos2 = make_annos(filtered_dog_dets)\n",
    "\n",
    "# # unscale annotations to draw in original image frame\n",
    "dog_unscaled2 = unscale_annos(dog_annos2, dw, dh, w0, h0, w1, h1)\n",
    "\n",
    "# # draw list of annotations on original image\n",
    "dog_annotated2 = draw_annos(dog_raw, dog_unscaled2, coco_names)\n",
    "\n",
    "print(f\"There are {len(dog_annos2)} filtered detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dog_annotated2[...,::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in a new image first\n",
    "# new image\n",
    "kite = arr1 = cv2.imread('../data/kite.jpg')\n",
    "kite = kite[..., ::-1] # BGR --> RGB\n",
    "plt.imshow(kite)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h1_k, w1_k, c1_k) = kite.shape\n",
    "(letterboxed_kite, (dw_k, dh_k), (w0_k, h0_k)) = letterbox(kite, (416, 416, 3))\n",
    "kite_results = run_inference(model, letterboxed_kite)\n",
    "\n",
    "OBJ_THRESH = 0.05\n",
    "CLASS_THRESH = 0.05\n",
    "IOU_THRESH = 0.3\n",
    "kite_dets = proc_results(kite_results, coco_names, pobj_thresh=OBJ_THRESH, pcls_thresh=CLASS_THRESH)\n",
    "filtered_kite_dets = basic_nms(kite_dets, IOU_THRESH)\n",
    "kite_annos = make_annos(filtered_kite_dets)\n",
    "\n",
    "# unscale annotations to draw in original image frame\\n\",\n",
    "kite_unscaled = unscale_annos(kite_annos, dw_k, dh_k, w0_k, h0_k, w1_k, h1_k)\n",
    "# draw list of annotations on original image\n",
    "kite_annotated = draw_annos(kite, kite_unscaled, coco_names)\n",
    "print(f\"There are {len(kite_annos)} filtered detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kite_annotated)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Object Detection on the Live Camera Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cons ONNX Tiny YOLOv3 NN model\n",
    "onnx_model_path = '../model/modified_yolov3-tiny.onnx'\n",
    "infer_sess = ort.InferenceSession(onnx_model_path)\n",
    "\n",
    "# create a config with desired attributes: format, size, framerate\n",
    "# NOTE: camera resolution 4608x2464, downsamples at 2304x1296 (56.03 fps)\n",
    "# NOTE: XRGB8888 => shape: (height, width, 4); pixel value: [B, G, R, A]\n",
    "config = picam2.create_preview_configuration(\n",
    "    main={'format': 'XRGB8888', 'size': (2304, 1296)})  # 16:9 aspect ratio\n",
    "\n",
    "# set camera configuration, start camera\n",
    "picam2.configure(config)\n",
    "picam2.start()\n",
    "\n",
    "# start opencv window thread\n",
    "cv2.startWindowThread()\n",
    "wnd_name = 'foo'\n",
    "cv2.namedWindow(wnd_name, cv2.WINDOW_KEEPRATIO)\n",
    "cv2.resizeWindow(wnd_name, 1600, 900)                   # 16:9 aspect ratio\n",
    "t0 = timeit.default_timer()\n",
    "t1 = timeit.default_timer()\n",
    "\n",
    "while True:\n",
    "    # update old time\n",
    "    t0 = t1\n",
    "\n",
    "    # get current image data from 'main' camera stream\n",
    "    arr1 = picam2.capture_array('main')\n",
    "    (h1, w1, c1) = arr1.shape\n",
    "\n",
    "    # letterbox the image to resize for NN input (size: (height, width, chan))\n",
    "    (arr2, (dw, dh), (w0, h0)) = letterbox(arr1, (416, 416, 4))\n",
    "\n",
    "    # cons packed input buffer and dims for ONNX model inference\n",
    "    arr3 = pack_buffer(arr2)\n",
    "\n",
    "    # run ONNX model inference on input buffer to get results\n",
    "    res = infer_sess.run(None, {'input_1': arr3})\n",
    "\n",
    "    # process results to make list of annotations\n",
    "    # TODO: change pobj_thresh, pcls_thresh, and iou_thresh\n",
    "    dets = proc_results(res, coco_names, 0.1, 0.5)\n",
    "    filtered_dets = basic_nms(dets, 0.5)\n",
    "    filtered_annos = make_annos(filtered_dets)\n",
    "\n",
    "    # unscale annotations to draw in original image frame\n",
    "    unscaled = unscale_annos(filtered_annos, dw, dh, w0, h0, w1, h1)\n",
    "\n",
    "    # draw list of annotations on original image\n",
    "    arr4 = draw_annos(arr1, unscaled, coco_names)\n",
    "\n",
    "    # update fps timer\n",
    "    t1 = timeit.default_timer()\n",
    "    fps = 1.0 / (t1 - t0)\n",
    "\n",
    "    # if window closed, break loop before imshow creates new window\n",
    "    if cv2.getWindowProperty(wnd_name, cv2.WND_PROP_AUTOSIZE) == -1:\n",
    "        break\n",
    "\n",
    "    # show annotated image\n",
    "    cv2.setWindowTitle(wnd_name, f'FPS: {fps:.1f}')\n",
    "    cv2.imshow(wnd_name, arr4)\n",
    "    #cv2.waitKey(1)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# stop camera\n",
    "cv2.destroyWindow(wnd_name)\n",
    "picam2.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
